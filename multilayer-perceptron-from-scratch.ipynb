{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPldsJlQ9kLZ0okAqVC8xfo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldor07/understainding-transformers/blob/main/multilayer-perceptron-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6CGsfwCGLJr",
        "outputId": "d9ba12f6-d95f-40bc-8746-5df0c9838a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Network class is used to represent a neural network\n",
        "class Network(object):\n",
        "\n",
        "  # initialising Network class\n",
        "\n",
        "  def __init__(self, sizes):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    # randomly intiallising weights and biases, except for the first layer which is the input layer\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "    # weights is a matrix representing weights of connections between two layers\n",
        "    self.weights = [np.random.randn(y, x)\n",
        "                    for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "  # given an input, feeds the output of each layer to the next and to produce the corresponding output layer\n",
        "  def feedforward(self, a):\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      # as mentioned, a is a vector/matrix containing activations of the previous layer and w is a vector/matrix containing the weights of all those connections\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "    return a\n",
        "\n",
        "  def stochastic_gradient_descent(self, training_data, epochs, mini_batch_size,eta,test_data = None):\n",
        "    # training data = tuples of (x, y) representing the training inputs and the desired output\n",
        "    # we're not testing rn so test_data = none, but if it's provided we evaluate the trianing output against it\n",
        "    if test_data:\n",
        "      n_test = len(test_data)\n",
        "\n",
        "    n = len.training_data\n",
        "\n",
        "    for j in np.xrange(epochs):\n",
        "        # epoch is a training cycle for the entire dataset\n",
        "        # in each epoch we randomly shuffle the dataset and create mini batches\n",
        "        np.random.shuffle(training_data)\n",
        "        mini_batches = [\n",
        "            training_data[k:k + mini_batch_size]\n",
        "            for k in np.xrange(0, n, mini_batch_size)\n",
        "        ]\n",
        "        # for each mini batch we apply a single step of gradient descent\n",
        "        for mini_batch in mini_batches:\n",
        "          # update_mini_batch updates the weights and biases according to the iteration\n",
        "          self.update_mini_batch(mini_batch, eta)\n",
        "        if test_data:\n",
        "            print (f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
        "        else:\n",
        "            print (f\"Epoch {j} complete\")\n",
        "\n",
        "    # updating the weights and biases of network, based on gradient calculated during backprop\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "\n",
        "      # initialised list of arrays with same shape as biases and weights\n",
        "      # stores the gradients of the cost function w.r.t biases and weights\n",
        "      nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "      nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "      for x,y in mini_batch:\n",
        "        # x is input data, y is corresponding output data\n",
        "        # backprop retursn gradients of cost fucntion w respect to bias and weight\n",
        "        delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "        nable_b = [nb + dnb for nb, dnb in zip(nabla_b ,delta_nabla_b)]\n",
        "        nable_w = [nw + dnw for nw, dnw in zip(nabla_w ,delta_nabla_w)]\n",
        "\n",
        "      # For each weight and bias, the corresponding gradient is multiplied by the learning rate (eta) divided by the size of the mini-batch.\n",
        "      self.weights = [w-(eta/len(mini_batch)) * nw\n",
        "                      for w, nw in zip(self.weights, nabla_w)]\n",
        "      self.biases = [b-(eta/len(mini_batch)) * nb\n",
        "                      for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "      \"\"\" Returns a tuple (nabla_b, nabla_w) representing the gradient for the cost function\n",
        "      nabla_b and nabla_w are layer by layer lists of numpy arrays similar to self.biases and self.weights\"\"\"\n",
        "\n",
        "      nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "      nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "      # feed forward\n",
        "      activation = x\n",
        "      # activations is a list htat will store the activations of all layers during the forward pass\n",
        "      activations = [x]\n",
        "      # zs is an empty list that will store the weightsed inputs of all layers during the forward pass\n",
        "      zs = []\n",
        "\n",
        "      # forward pass of the nerual network\n",
        "      # for each layer the weighted input z is calcualted by taking the dot product of the weight w and activation of previous layer and add corr bias b\n",
        "      for b, w in zip(self.biases, self.weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        zs.append(z)\n",
        "        acivation = sigmoid(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "      # backward pass\n",
        "      delta = self.cost_derivative(activations[-1], y) * \\\n",
        "          sigmoid_prime(zs[-1])\n",
        "      nabla_b[-1] = delta\n",
        "      nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "      for l in xrange(2, self.num_layers):\n",
        "          z = zs[-l]\n",
        "          sp = sigmoid_prime(z)\n",
        "          delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "          nabla_b[-l] = delta\n",
        "          nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "      return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Return the number of test inputs for which the neural\n",
        "        network outputs the correct result. Note that the neural\n",
        "        network's output is assumed to be the index of whichever\n",
        "        neuron in the final layer has the highest activation.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
        "        \\partial a for the output activations.\"\"\"\n",
        "        return (output_activations-y)\n",
        "# sigmoid function for squishification\n",
        "def sigmoid(x):\n",
        "  return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "Bgn6kj7sMh6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network([2,3,1])"
      ],
      "metadata": {
        "id": "emV9lD5GNVLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}