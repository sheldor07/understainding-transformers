{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNINCCEgD8Va2O1BYgVm2mZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldor07/understainding-transformers/blob/main/multilayer-perceptron-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U6CGsfwCGLJr",
        "outputId": "601fb2bc-2cb4-4e8d-f161-771237320b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "----------------\n",
            "(60000,)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print('----------------')\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshaping the data to fit our input layer\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)/255.0\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)/255.0\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vhgb7o3gsBe",
        "outputId": "c05d3ef5-d2d5-4414-bfdd-4db70b89950c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding for labels\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh4l5sIOhEMh",
        "outputId": "5683b772-f028-44cf-e813-e62a4d170456"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10)\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
        "\n",
        "        # initializing the instance variables\n",
        "        self.input_neurons = input_neurons\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.output_neurons = output_neurons\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Links of weights from input layer to hidden layer\n",
        "        self.wih = np.random.normal(0.0, pow(self.input_neurons, -0.5), (self.hidden_neurons, self.input_neurons))\n",
        "        self.bih = 0\n",
        "\n",
        "        # Links of weights from hidden layer to output layer\n",
        "        self.who = np.random.normal(0.0, pow(self.hidden_neurons, -0.5), (self.output_neurons, self.hidden_neurons))\n",
        "        self.bho = 0\n",
        "\n",
        "        self.lr = learning_rate # Learning rate\n",
        "\n",
        "    def activation(self, z):\n",
        "        \"\"\"Returns the sigmoid of z\"\"\"\n",
        "        z = np.clip(z, -500, 500) # Avoid overflow error\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        \"\"\"Returns the derivative of the sigmoid of z\"\"\"\n",
        "        return self.activation(z) * (1 - self.activation(z))\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, input_list):\n",
        "        inputs = np.array(input_list, ndmin=2).T\n",
        "\n",
        "        # Passing inputs to the hidden layer\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "\n",
        "        # Getting outputs from the hidden layer\n",
        "        hidden_outputs = self.activation(hidden_inputs)\n",
        "\n",
        "        # Passing inputs from the hidden layer to the output layer\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "\n",
        "        # Getting output from the output layer\n",
        "        yj = self.activation(final_inputs)\n",
        "\n",
        "        return yj\n",
        "\n",
        "\n",
        "    # Back propagation\n",
        "    def backprop(self, inputs_list, targets_list):\n",
        "\n",
        "        inputs = np.array(inputs_list, ndmin=2).T\n",
        "\n",
        "        tj = np.array(targets_list, ndmin=2).T # Targets\n",
        "        # passing inputs to the hidden layer\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "\n",
        "        # Getting outputs from the hidden layer\n",
        "        hidden_outputs = self.activation(hidden_inputs)\n",
        "\n",
        "        # Passing inputs from the hidden layer to the output layer\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "\n",
        "        # Getting output from the output layer\n",
        "        yj = self.activation(final_inputs)\n",
        "\n",
        "        # Finding the errors from the output layer\n",
        "        output_errors = -(tj - yj)\n",
        "\n",
        "        # Finding the error in the hidden layer\n",
        "        hidden_errors = np.dot(self.who.T, output_errors)\n",
        "\n",
        "        # Updating the weights using Update Rule\n",
        "        self.who -= self.lr * np.dot((output_errors * self.sigmoid_derivative(yj)), np.transpose(hidden_outputs))\n",
        "        self.wih -= self.lr * np.dot((hidden_errors * self.sigmoid_derivative(hidden_outputs)), np.transpose(inputs))\n",
        "\n",
        "\n",
        "        #updating bias\n",
        "        self.bho -= self.lr * (output_errors * self.sigmoid_derivative(yj))\n",
        "        self.bih -= self.lr * (hidden_errors * self.sigmoid_derivative(hidden_outputs))\n",
        "        pass\n",
        "\n",
        "    # Performing Gradient Descent Optimization using Backpropagation\n",
        "    def fit(self, inputs_list, targets_list):\n",
        "        for epoch in range(self.epochs):\n",
        "            self.backprop(inputs_list, targets_list)\n",
        "            print(f\"Epoch {epoch}/{self.epochs} completed.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X).T\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Bgn6kj7sMh6e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NN(input_neurons=784, hidden_neurons=64, output_neurons=10, learning_rate=0.01, epochs=100)\n",
        "nn.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFkagpK0mPi_",
        "outputId": "1646dce6-2f8f-4ce6-acc3-7bf6c6dbcf95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100 completed.\n",
            "Epoch 1/100 completed.\n",
            "Epoch 2/100 completed.\n",
            "Epoch 3/100 completed.\n",
            "Epoch 4/100 completed.\n",
            "Epoch 5/100 completed.\n",
            "Epoch 6/100 completed.\n",
            "Epoch 7/100 completed.\n",
            "Epoch 8/100 completed.\n",
            "Epoch 9/100 completed.\n",
            "Epoch 10/100 completed.\n",
            "Epoch 11/100 completed.\n",
            "Epoch 12/100 completed.\n",
            "Epoch 13/100 completed.\n",
            "Epoch 14/100 completed.\n",
            "Epoch 15/100 completed.\n",
            "Epoch 16/100 completed.\n",
            "Epoch 17/100 completed.\n",
            "Epoch 18/100 completed.\n",
            "Epoch 19/100 completed.\n",
            "Epoch 20/100 completed.\n",
            "Epoch 21/100 completed.\n",
            "Epoch 22/100 completed.\n",
            "Epoch 23/100 completed.\n",
            "Epoch 24/100 completed.\n",
            "Epoch 25/100 completed.\n",
            "Epoch 26/100 completed.\n",
            "Epoch 27/100 completed.\n",
            "Epoch 28/100 completed.\n",
            "Epoch 29/100 completed.\n",
            "Epoch 30/100 completed.\n",
            "Epoch 31/100 completed.\n",
            "Epoch 32/100 completed.\n",
            "Epoch 33/100 completed.\n",
            "Epoch 34/100 completed.\n",
            "Epoch 35/100 completed.\n",
            "Epoch 36/100 completed.\n",
            "Epoch 37/100 completed.\n",
            "Epoch 38/100 completed.\n",
            "Epoch 39/100 completed.\n",
            "Epoch 40/100 completed.\n",
            "Epoch 41/100 completed.\n",
            "Epoch 42/100 completed.\n",
            "Epoch 43/100 completed.\n",
            "Epoch 44/100 completed.\n",
            "Epoch 45/100 completed.\n",
            "Epoch 46/100 completed.\n",
            "Epoch 47/100 completed.\n",
            "Epoch 48/100 completed.\n",
            "Epoch 49/100 completed.\n",
            "Epoch 50/100 completed.\n",
            "Epoch 51/100 completed.\n",
            "Epoch 52/100 completed.\n",
            "Epoch 53/100 completed.\n",
            "Epoch 54/100 completed.\n",
            "Epoch 55/100 completed.\n",
            "Epoch 56/100 completed.\n",
            "Epoch 57/100 completed.\n",
            "Epoch 58/100 completed.\n",
            "Epoch 59/100 completed.\n",
            "Epoch 60/100 completed.\n",
            "Epoch 61/100 completed.\n",
            "Epoch 62/100 completed.\n",
            "Epoch 63/100 completed.\n",
            "Epoch 64/100 completed.\n",
            "Epoch 65/100 completed.\n",
            "Epoch 66/100 completed.\n",
            "Epoch 67/100 completed.\n",
            "Epoch 68/100 completed.\n",
            "Epoch 69/100 completed.\n",
            "Epoch 70/100 completed.\n",
            "Epoch 71/100 completed.\n",
            "Epoch 72/100 completed.\n",
            "Epoch 73/100 completed.\n",
            "Epoch 74/100 completed.\n",
            "Epoch 75/100 completed.\n",
            "Epoch 76/100 completed.\n",
            "Epoch 77/100 completed.\n",
            "Epoch 78/100 completed.\n",
            "Epoch 79/100 completed.\n",
            "Epoch 80/100 completed.\n",
            "Epoch 81/100 completed.\n",
            "Epoch 82/100 completed.\n",
            "Epoch 83/100 completed.\n",
            "Epoch 84/100 completed.\n",
            "Epoch 85/100 completed.\n",
            "Epoch 86/100 completed.\n",
            "Epoch 87/100 completed.\n",
            "Epoch 88/100 completed.\n",
            "Epoch 89/100 completed.\n",
            "Epoch 90/100 completed.\n",
            "Epoch 91/100 completed.\n",
            "Epoch 92/100 completed.\n",
            "Epoch 93/100 completed.\n",
            "Epoch 94/100 completed.\n",
            "Epoch 95/100 completed.\n",
            "Epoch 96/100 completed.\n",
            "Epoch 97/100 completed.\n",
            "Epoch 98/100 completed.\n",
            "Epoch 99/100 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting probabilities\n",
        "probs = []\n",
        "for sample in x_test:\n",
        "    prob = nn.predict(sample)\n",
        "    probs.append(prob)\n",
        "\n",
        "# Converting probabilities to one-hot vector format\n",
        "predictions = []\n",
        "for prob in probs:\n",
        "    max_idx = np.argmax(prob)\n",
        "    prediction = np.zeros_like(prob)\n",
        "    prediction[max_idx] = 1\n",
        "    predictions.append(prediction)\n"
      ],
      "metadata": {
        "id": "ldzy7LZMKMe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "print(\"Accuracy:\",accuracy_score(predictions, y_test))\n",
        "print(\"CR:\", classification_report(predictions, y_test))"
      ],
      "metadata": {
        "id": "ReMUDVunJEfV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}