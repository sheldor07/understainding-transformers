{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrjnsZjhdWi4CE3QH2zQgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldor07/understainding-transformers/blob/main/multilayer-perceptron-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U6CGsfwCGLJr",
        "outputId": "d9ba12f6-d95f-40bc-8746-5df0c9838a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Network class is used to represent a neural network\n",
        "class Network(object):\n",
        "\n",
        "  # initialising Network class\n",
        "\n",
        "  def __init__(self, sizes):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    # randomly intiallising weights and biases, except for the first layer which is the input layer\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "    # weights is a matrix representing weights of connections between two layers\n",
        "    self.weights = [np.random.randn(y, x)\n",
        "                    for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "  # sigmoid function for squishification\n",
        "  def sigmoid(x):\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "  # given an input, feeds the output of each layer to the next and to produce the corresponding output layer\n",
        "  def feedforward(self, a):\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      # as mentioned, a is a vector/matrix containing activations of the previous layer and w is a vector/matrix containing the weights of all those connections\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "    return a\n",
        "\n",
        "  def stochastic_gradient_descent(self, training_data, epochs, mini_batch_size,eta,test_data = None):\n",
        "    # training data = tuples of (x, y) representing the training inputs and the desired output\n",
        "    # we're not testing rn so test_data = none, but if it's provided we evaluate the trianing output against it\n",
        "    if test_data:\n",
        "      n_test = len(test_data)\n",
        "\n",
        "    n = len.training_data\n",
        "\n",
        "    for j in np.xrange(epochs):\n",
        "        # epoch is a training cycle for the entire dataset\n",
        "        # in each epoch we randomly shuffle the dataset and create mini batches\n",
        "        np.random.shuffle(training_data)\n",
        "        mini_batches = [\n",
        "            training_data[k:k + mini_batch_size]\n",
        "            for k in np.xrange(0, n, mini_batch_size)\n",
        "        ]\n",
        "        # for each mini batch we apply a single step of gradient descent\n",
        "        for mini_batch in mini_batches:\n",
        "          # update_mini_batch updates the weights and biases according to the iteration\n",
        "          self.update_mini_batch(mini_batch, eta)\n",
        "        if test_data:\n",
        "            print (f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
        "        else:\n",
        "            print (f\"Epoch {j} complete\")\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "\n",
        "      nabla_b ="
      ],
      "metadata": {
        "id": "Bgn6kj7sMh6e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network([2,3,1])"
      ],
      "metadata": {
        "id": "emV9lD5GNVLN"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}